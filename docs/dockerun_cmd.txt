--------------------------------------------------------------
Docker Image creation with Spark download:
---------------------------------------------------------------
>> Download the docker image for PySpark
    docker pull apache/spark-py:latest

>> Use the Docker Run Command to create a container with your workspace code:
    docker run -dt -v  C:/Workspaces/GitHub/QE_ATF/:/app  --name DATF apache/spark-py bash

>> use container name to access bash
    docker exec -ti -u root DATF bash

>> within the folder "add/scripts" run a bash script
    sh install.sh
    
-------------------------------------------------------------------
Start Execution with Test Cases either 'all' or seperated by comma:
-------------------------------------------------------------------

    sh testingstart.sh count <testcasenames>

    sh testingstart.sh content <testcasenames>

    sh testingstart.sh duplicate <testcasenames>